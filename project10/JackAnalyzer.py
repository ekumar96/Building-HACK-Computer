# -*- coding: utf-8 -*-
"""JackAnalyzer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xYg9bEy3rv4vP7Z7KhoKVVknFTonvPR0
    - added few lines when translating from .ipynb -> .py, these are marked with #.py added line

# Jack Analyzer for Nand2Tetris, Project 10
Purpose: generate an XML file from a given .jack file (similar to .java file). We choose to output an XML file in order to ensure we are getting some "structured syntax" of the source code, providing evidence that our parser "understands" the input file. 

This is the first stage of compilation, when  the syntax is analyzed. The Syntax analyzer, has two components, the tokenizer and the parser. They are described below.

# Pre-processing input
First, the file is opened and it's contents are read as a string. Then comments are removed, and extra spaces are removed.
"""

#import sys, os added for files .py added line
import re
import sys, os

def getFileString(fileName):
  with open(fileName, 'r') as reader:
      # Read & print the entire file
      jackString = reader.read()
      #print(asmString)
  return jackString

def writeStringFile(fileName, xmlStr):
  with open(fileName+'.xml', 'w') as fileW:
      # Read & print the entire file
      n = fileW.write(xmlStr)
      #print(asmString)
  return

def removeComments(string): 
    #comment_re = re.compile(r'(^)?[^\S\n]*/(?:\*(.*?)\*/[^\S\n]*|/[^\n]*)($)?', re.DOTALL | re.MULTILINE )
    #string = re.sub(comment_re ,"" ,string)
    string = re.sub(re.compile("/\*[\s\S]*?\*/",re.DOTALL | re.MULTILINE ) ,"" ,string) # remove all occurrences streamed comments (/*COMMENT */) from string
    string = re.sub(re.compile("//[\s\S]*?$", re.DOTALL | re.MULTILINE) ,"" ,string) # remove all occurrence single-line comments (//COMMENT\n ) from string
    string = re.sub("[\s]+" ," ",string) #all multi-spaces are replaced with a single space
    lineList = string.split('\n') #split lines by newline
    cleanedLines = []
    for line in lineList:
      if (line): #remove empty lines
          cleanedLines.append(line)
    cleanFile = ''.join(cleanedLines)+" " #put all lines together as single string, with one space between each line
    return cleanFile

"""#Tokenizing input
First, the input is tokenized. This means that it seperates the input into "tokens". Each token has a value (eg. "static") and type (in angular brackets eg. \<keyword\>). The tokenizer simply takes the input, and seperates it into a series of tokens, one per line. However, this is not as easy as delimiting by spaces, because eg. x=x+1; should be read as 6 tokens.
"""

def JackTokenizer(input):
  #in order for the last function (evaluating identifiers) to work, needs a space at the end
  keywords = ["class", "constructor", "function", "method", "field", "static", "var", "int", "char", "boolean", "void", "true", "false", "null", "this", "let", "do", "if", "else", "while", "return"]
  symbols = ["{", "}", "[", "]", "(", ")", ".", ",", ";", "+", "-", "*", "/", "&", "|", "<", ">", "=", "~"]

  input = input+" "
  currToken = ""
  pushBack = ""
  isString = False
  tokenList = ["<tokens>"]
  while(input): 
    #reads input character by character
    nextChar = input[0]
    if nextChar == "\"" and not currToken: 
      #strings are handled differently - anything goes in a string, including spaces, integers, etc.
      isString = True;
    if isString:
      currToken = currToken+nextChar #will add anything to the token in a string
      #if realizes that has come to end of string (sees second "), ends string, resets token, and adds string token to tokenList
      if (currToken[0]=="\"" and currToken[-1]=="\"" and len(currToken) > 1):
        tokenList.append("<stringConstant> "+currToken[1:-1]+" </stringConstant>")
        currToken = ""
        isString = False;
    elif nextChar != ' ' and not isString:
      currToken = currToken+nextChar
      if currToken in keywords:
        #reading character by character, tries to see if character is in keywords, symbols, or if its a number
        tokenList.append("<keyword> "+currToken+ " </keyword>")
        currToken = ""
      elif currToken in symbols:
        if currToken == ">":
          #for certain characters, special symbols need to be appended as tokens (this is because the angular brackets are used to denote token type)
          tokenList.append("<symbol> &gt; </symbol>")
        elif currToken == "<":
          tokenList.append("<symbol> &lt; </symbol>")
        elif currToken == "&":
          tokenList.append("<symbol> &amp; </symbol>")
        else:
          tokenList.append("<symbol> "+currToken+" </symbol>")
        currToken = ""
      elif (currToken.isnumeric() and not (input[1].isnumeric())):
        #if a number has ended, it can be added as a token
        tokenList.append("<integerConstant> "+currToken+" </integerConstant>")
        currToken = ""
    elif currToken and not isString:
      #if has reached a " ", must be either an identifier (variable/function/method name) or an identifier followed by other tokens (eg. x=x+1; )
      pushBack = ""
      for i in range(len(currToken)):
        #we know that if anything is directly adjacent to a variable without a space, it must be a symbol 
        #so we check if there are any symbols in the current token
        if currToken[i] in symbols:
          #if so, we save everything after the symbol to push back, 
          #and we take everything up until the symbol as the current token
          pushBack = currToken[i-1:]
          currToken = currToken[:i]
          break
      tokenList.append("<identifier> "+currToken+" </identifier>")
      #everything after the symbol (inclusive) gets pushed back into the input string to be processed again
      input = pushBack+input
      currToken = "" #resets current token
    input = input[1:] #takes out first character, trimming input
  tokenList.append("</tokens>")
  tokenFile = '\n'.join(tokenList)
  return tokenList, tokenFile

"""# Parser
Next, the tokens must be parsed. This means that we must output some type of structured syntax, which we can do in the form of a parse tree in an XML file. We must construct this XML file in accordance with the Jack Syntax. Eg. "if" must be followed by a "(" which must be followed by an "expression", which must be followed by a ")", then a "{\", then some statements (do to tasks, assign variables, or call functions), and then a "}". The whole file can be structured like this according to rules, called the "Grammar" of the language. 

We do this with many functions that can call each other, and act recursively, to construct the parse tree with the tokens as input
"""

def eat(targetStrings, tokens, parsedLines, optional, level):
  #function that will see if target strings (list of strings) is in any part of token
  for targetString in targetStrings:
    if tokens and targetString in tokens[0]:
      #if so, appends token to the output file (parsed Lines) and removes from input
      parsedLines.append("  "*level+tokens[0])
      tokens.pop(0)
      return tokens, parsedLines, True
  if not optional:
    #if not target strings are not in the token, but must be, optional will be set to false, and an error will be raised
    print('\n'.join(parsedLines))
    raise SyntaxError("Error, missing "+targetString)
  return tokens, parsedLines, False

def compilationEngine(tokens):
  temptokens = tokens.copy()
  parsedLines = []
  level = 0
  #main function that calls the first compiler of any jack file, the class
  compileClass(temptokens, parsedLines, level)
  return parsedLines

def compileClass(tokens, parsedLines, level):
  #level argument allows for proper indentation, 2 spaces per level
  parsedLines.extend(["  "*level+"<class>"]) #notation for XML
  classVarNext = True
  subRoutineNext = True
  eat(["class"], tokens, parsedLines, False, level+1) 
  #false for these indicate that these are mandatory strings that must be eaten, if they are not here an error will be raised
  eat(["identifier"], tokens, parsedLines, False, level+1)
  eat(["{"], tokens, parsedLines, False, level+1)
  while classVarNext:
    #keeps checking for multiple class variable declarations and then for multiple subroutines
    parsedLines, classVarNext = compileClassVarDec(tokens, parsedLines, True, level+1)
  while subRoutineNext:
    parsedLines, subRoutineNext = compileSubroutineDec(tokens, parsedLines, True, level+1)
  eat(["}"], tokens, parsedLines, False, level+1)

  parsedLines.extend(["  "*level+"</class>"])
  return parsedLines

def compileClassVarDec(tokens, parsedLines, optional, level):
  if "static" in tokens[0] or "field" in tokens[0]:
    parsedLines.extend(["  "*level+"<classVarDec>"])
    eat(["static", "field"], tokens, parsedLines, False, level+1)
    #can eat static or field, whichever is in token
    eat(["int", "char", "boolean", "identifier"], tokens, parsedLines, False, level+1)
    eat(["identifier"], tokens, parsedLines, False, level+1)
    while "," in tokens[0]:
      #can ask for list of variables to declare
      eat([","], tokens, parsedLines, True, level+1)
      eat(["identifier"], tokens, parsedLines, True, level+1)
    eat([";"], tokens, parsedLines, False, level+1)
    parsedLines.extend(["  "*level+"</classVarDec>"])
    return parsedLines, True
  elif optional:
    return parsedLines, False
    #if this was an optional peek, simply return
  else:
    #if this was not optional and the above was not found, raise an error
    print("classvardec ")
    print(tokens)
    raise SyntaxError()

def compileSubroutineDec(tokens, parsedLines, optional, level):
  if "constructor" in tokens[0] or "function" in tokens[0] or "method" in tokens[0]:
    #similar logic to above, if not optional, must start with subroutine names
    parsedLines.extend(["  "*level+"<subroutineDec>"])
    eat(["constructor", "function", "method"], tokens, parsedLines, False, level+1)
    eat(["void", "int", "char", "boolean", "identifier"], tokens, parsedLines, False, level+1)
    eat(["identifier"], tokens, parsedLines, False, level+1)
    eat(["("], tokens, parsedLines, False, level+1)
    compileParameterList(tokens, parsedLines, level+1)
    eat([")"], tokens, parsedLines, False, level+1)
    compileSubroutineBody(tokens, parsedLines, level+1)
    parsedLines.extend(["  "*level+"</subroutineDec>"])
    return parsedLines, True
  elif optional:
    return parsedLines, False
  else:
    print("subroutine ")
    print(tokens)
    raise SyntaxError()


def compileParameterList(tokens, parsedLines, level):
  parsedLines.extend(["  "*level+"<parameterList>"])
  eat(["int", "char", "boolean", "identifier"], tokens, parsedLines, True, level+1)
  eat(["identifier"], tokens, parsedLines, True, level+1)
  while "," in tokens[0]:
    #continues to look for multiple parameters if more than 1, but if enter here then must eat (optional set to false)
    eat([","], tokens, parsedLines, False, level+1)
    eat(["int", "char", "boolean", "identifier"], tokens, parsedLines, False, level+1)
    eat(["identifier"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</parameterList>"])
  return parsedLines

def compileSubroutineBody(tokens, parsedLines, level):
  varNext = True
  parsedLines.extend(["  "*level+"<subroutineBody>"])
  eat(["{"], tokens, parsedLines, False, level+1)
  while varNext:
    #tries to find multiple variables, but is optional 
    parsedLines, varNext = compileVarDec(tokens, parsedLines, True, level+1)
  compileStatements(tokens, parsedLines, True, level+1) #tries to find multiple statements
  eat(["}"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</subroutineBody>"])
  return parsedLines

def compileVarDec(tokens, parsedLines, optional, level):
  if "var" in tokens[0]:
    #similar to class var dec, tries to find multiple variables
    parsedLines.extend(["  "*level+"<varDec>"])
    eat(["var"], tokens, parsedLines, False, level+1)
    eat(["int", "char", "boolean", "identifier"], tokens, parsedLines, False, level+1)
    eat(["identifier"], tokens, parsedLines, False, level+1)
    while "," in tokens[0]:
      eat([","], tokens, parsedLines, True, level+1)
      eat(["identifier"], tokens, parsedLines, True, level+1)
    eat([";"], tokens, parsedLines, False, level+1)
    parsedLines.extend(["  "*level+"</varDec>"])
    return parsedLines, True
  elif optional:
    return parsedLines, False
  else:
    print("VarDec ")
    print(tokens)
    raise SyntaxError()

def compileStatements(tokens, parsedLines, optional, level):
  parsedLines.extend(["  "*level+"<statements>"])
  statementExists = False
  #tries to append as many statements as exist in the tokens, each statement type has own function
  while "let" in tokens[0] or "if" in tokens[0] or "while" in tokens[0] or "do" in tokens[0] or "return" in tokens[0]:
    statementExists = True
    if "let" in tokens[0]:
      compileLet(tokens, parsedLines, level+1)
    elif "if" in tokens[0]:
      compileIf(tokens, parsedLines, level+1)
    elif"while" in tokens[0]:
      compileWhile(tokens, parsedLines, level+1)
    elif "do" in tokens[0]: 
      compileDo(tokens, parsedLines, level+1)
    elif "return" in tokens[0]:
      compileReturn(tokens, parsedLines, level+1)
  if not statementExists and not optional:
    #if there must be statements, but no statement was found, raise an error
    print("statements ")
    print('\n'.join(parsedLines))
    print(tokens)
    raise SyntaxError()
  parsedLines.extend(["  "*level+"</statements>"])
  statementExists = False
  return parsedLines


def compileLet(tokens, parsedLines, level):
  parsedLines.extend(["  "*level+"<letStatement>"])
  eat(["let"], tokens, parsedLines, False, level+1)
  eat(["identifier"], tokens, parsedLines, False, level+1)
  tokens, parsedLines, eaten = eat(["["], tokens, parsedLines, True, level+1)
  if eaten:
    #if found left square bracket, must find an expression followed by the right square bracket (array)
    compileExpression(tokens, parsedLines, False, level+1)
    eat(["]"], tokens, parsedLines, False, level+1)
  #let statement must have an =expression;
  eat(["="], tokens, parsedLines, False, level+1)
  compileExpression(tokens, parsedLines, False, level+1)
  eat([";"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</letStatement>"])
  return parsedLines

def compileIf(tokens, parsedLines, level):
  parsedLines.extend(["  "*level+"<ifStatement>"])
  eat(["if"], tokens, parsedLines, False, level+1)
  eat(["("], tokens, parsedLines, False, level+1)
  compileExpression(tokens, parsedLines, False, level+1)
  #if the previous expresion, do the statement below
  eat([")"], tokens, parsedLines, False, level+1)
  eat(["{"], tokens, parsedLines, False, level+1)
  compileStatements(tokens, parsedLines, True, level+1)
  eat(["}"], tokens, parsedLines, False, level+1)
  if "else" in tokens[0]:
    #optional else statement, but if so must follow structure
    eat(["else"], tokens, parsedLines, False, level+1)
    eat(["{"], tokens, parsedLines, False, level+1)
    #if can recursively have more statements (let, do etc. )
    compileStatements(tokens, parsedLines, True, level+1)
    eat(["}"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</ifStatement>"])
  return parsedLines

def compileWhile(tokens, parsedLines, level):
  parsedLines.extend(["  "*level+"<whileStatement>"])
  eat(["while"], tokens, parsedLines, False, level+1)
  eat(["("], tokens, parsedLines, False, level+1)
  compileExpression(tokens, parsedLines, False, level+1)
  eat([")"], tokens, parsedLines, False, level+1)
  eat(["{"], tokens, parsedLines, False, level+1)
  compileStatements(tokens, parsedLines, False, level+1)
  eat(["}"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</whileStatement>"])
  return parsedLines

def compileDo(tokens, parsedLines, level):
  parsedLines.extend(["  "*level+"<doStatement>"])
  eat(["do"], tokens, parsedLines, False, level+1)
  #if statement for subroutine(expressionlist) (calling subroutine in current class)
  if "identifier" in tokens[0] and "(" in tokens[1]:
    eat(["identifier"], tokens, parsedLines, False, level+1)
    eat(["("], tokens, parsedLines, False, level+1)
    compileExpressionList(tokens, parsedLines, level+1)
    eat([")"], tokens, parsedLines, False, level+1)
  elif "identifier" in tokens[0] and "." in tokens[1]:
    #if not a simple subroutine call, must be a class.subroutine(expression) (calling subroutine from other class)
    eat(["identifier"], tokens, parsedLines, False, level+1)
    eat(["."], tokens, parsedLines, False, level+1)
    eat(["identifier"], tokens, parsedLines, False, level+1)
    eat(["("], tokens, parsedLines, False, level+1)
    compileExpressionList(tokens, parsedLines, level+1)
    eat([")"], tokens, parsedLines, False, level+1)
  else:
    #if neither of the statements, must be an error
    print("Do statement ")
    print(tokens)
    raise SyntaxError()
  eat([";"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</doStatement>"])
  return parsedLines

def compileReturn(tokens, parsedLines, level):
  #similar logic to above statements
  parsedLines.extend(["  "*level+"<returnStatement>"])
  eat(["return"], tokens, parsedLines, False, level+1)
  compileExpression(tokens, parsedLines, True, level+1)
  eat([";"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</returnStatement>"])
  return parsedLines

def compileExpression(tokens, parsedLines, optional, level):
  #complicated functions of analyzer
  if "integerConstant" in tokens[0] or "stringConstant" in tokens[0] or "keyword" in tokens[0] or "identifier" in tokens[0] or "(" in tokens[0] or "-" in tokens[0] or "~" in tokens[0]:
    parsedLines.extend(["  "*level+"<expression>"])
    #expression must start with term, then optional ("operation" followed by term)
    compileTerm(tokens, parsedLines, level+1)
    divlocation = tokens[0].find('/') #need to make special case for division, because "/" is in every token at the end (eg. <identifier> </identifier>)
    while tokens and ("+" in tokens[0] or "-" in tokens[0] or "*" in tokens[0] or tokens[0][divlocation-1] != '<' or "&amp" in tokens[0] or "|" in tokens[0] or "&lt" in tokens[0] or "&gt" in tokens[0] or "=" in tokens[0]):
      #if has found a / that is not preceded by a "<", is a genuine division
      eat(["+", "-", "*", "/", "&amp", "|", "&gt", "&lt", "="], tokens, parsedLines, False, level+1)
      compileTerm(tokens, parsedLines, level+1) #operation must be followed by a term
      divlocation = tokens[0].find('/') #must be reset for next term so not stuck in loop
    parsedLines.extend(["  "*level+"</expression>"])
    return parsedLines, True
  elif optional:
    return parsedLines, False
  else:
    print("Expression ")
    print(tokens)
    raise SyntaxError()

def compileTerm(tokens, parsedLines, level):
  parsedLines.extend(["  "*level+"<term>"])
  #many diff types of terms that can be operated on
  tokens, parsedLines, eaten = eat(["integerConstant", "stringConstant", "true", "false", "null", "this"], tokens, parsedLines, True, level+1)
  #if not one of the constant values above, tries other things
  if not eaten:
    if tokens and "(" in tokens[0]:
      #if starts with paranetheses, must be expression within 
      eat(["("], tokens, parsedLines, False, level+1)
      compileExpression(tokens, parsedLines, False, level+1)
      eat([")"], tokens, parsedLines, False, level+1)
    elif tokens and ("-" in tokens[0] or "~" in tokens[0]):
      #if starts with negation, must be followed by term
      eat(["-", "~"], tokens, parsedLines, False, level+1)
      compileTerm(tokens, parsedLines, level+1)
    elif tokens and ("identifier" in tokens[0] and "[" in tokens[1]):
      #if name followed by square bracket, must be array
      eat(["identifier"], tokens, parsedLines, False, level+1)
      eat(["["], tokens, parsedLines, False, level+1)
      compileExpression(tokens, parsedLines, False, level+1)
      eat(["]"], tokens, parsedLines, False, level+1)
    elif tokens and ("identifier" in tokens[0] and "(" in tokens[1]):
      #if identifier followed by paranthesis, must be subroutine call
      eat(["identifier"], tokens, parsedLines, False, level+1)
      eat(["("], tokens, parsedLines, False, level+1)
      compileExpressionList(tokens, parsedLines, level+1)
      eat([")"], tokens, parsedLines, False, level+1)
    elif tokens and "identifier" in tokens[0] and "." in tokens[1]:
      #if identifier followed by ., must be class.subtourine call (subroutine call from diff class)
      eat(["identifier"], tokens, parsedLines, False, level+1)
      eat(["."], tokens, parsedLines, False, level+1)
      eat(["identifier"], tokens, parsedLines, False, level+1)
      eat(["("], tokens, parsedLines, False, level+1)
      compileExpressionList(tokens, parsedLines, level+1)
      eat([")"], tokens, parsedLines, False, level+1)
    elif tokens and "identifier" in tokens[0]:
      #or could be simple variable
      eat(["identifier"], tokens, parsedLines, False, level+1)
  parsedLines.extend(["  "*level+"</term>"])
  return parsedLines

def compileExpressionList(tokens, parsedLines, level):
  parsedLines.extend(["  "*level+"<expressionList>"])
  #compiles single expression
  compileExpression(tokens, parsedLines, True, level+1)
  while "," in tokens[0]: #can compile list of expressions if this exists
    eat([","], tokens, parsedLines, True, level+1)
    compileExpression(tokens, parsedLines, True, level+1)
  parsedLines.extend(["  "*level+"</expressionList>"])
  return parsedLines


def main():
  #.py added 5 lines to get arguments when calling python3 in terminal, then removing .vm, then getting only file name
  args = sys.argv[1:] 
  inputFilePath = args[0] 
  files = []
  outputName = []
  """# Main
  The Main function, which gets the file as a string, removes the comments, tokenizes it, and compiles it to contruct an XML file/parse tree. 
  """

  if inputFilePath[-5:] == ".jack":
    fileName = inputFilePath.split('/')[-1]
    files.append(inputFilePath)
    dirPath = '/'.join(inputFilePath.split('/')[:-1])+'/'
    outputName.append(dirPath+fileName[:-5])
  else:
    jackFiles = [inputFilePath+'/'+f for f in os.listdir(inputFilePath) if f.endswith('.jack')]
    files.extend(jackFiles)
    outputName = [file[:-5] for file in files]

  for index, fileName in enumerate(files):
    fileString = getFileString(fileName)
    cleanFile = removeComments(fileString)

    tokens, tokenFile = JackTokenizer(cleanFile)

    ptokens = tokens[1:-1]
    parseTree = '\n'.join(compilationEngine(ptokens))
    writeStringFile(outputName[index], parseTree)


if __name__ == "__main__":
    main()
